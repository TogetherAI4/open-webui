# Open WebUI (Fr√ºher Ollama WebUI) üëã

![GitHub Sterne](https://img.shields.io/github/stars/open-webui/open-webui?style=social)
![GitHub Forks](https://img.shields.io/github/forks/open-webui/open-webui?style=social)
![GitHub Beobachter](https://img.shields.io/github/watchers/open-webui/open-webui?style=social)
![GitHub Repository Gr√∂√üe](https://img.shields.io/github/repo-size/open-webui/open-webui)
![GitHub Sprachenanzahl](https://img.shields.io/github/languages/count/open-webui/open-webui)
![GitHub Top-Sprache](https://img.shields.io/github/languages/top/open-webui/open-webui)
![GitHub letzter Commit](https://img.shields.io/github/last-commit/open-webui/open-webui?color=red)
![Hits](https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2Follama-webui%2Follama-wbui&count_bg=%2379C83D&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=hits&edge_flat=false)
[![Discord](https://img.shields.io/badge/Discord-Open_WebUI-blue?logo=discord&logoColor=white)](https://discord.gg/5rJgQTnV4s)
[![](https://img.shields.io/static/v1?label=Sponsor&message=%E2%9D%A4&logo=GitHub&color=%23fe8e86)](https://github.com/sponsors/tjbck)

Open WebUI ist eine [erweiterbare](https://github.com/open-webui/pipelines), funktionsreiche und benutzerfreundliche selbstgehostete WebUI, die vollst√§ndig offline betrieben werden kann. Sie unterst√ºtzt verschiedene LLM-Runner, darunter Ollama- und OpenAI-kompatible APIs. Weitere Informationen findest du in unserer [Open WebUI-Dokumentation](https://docs.openwebui.com/).

![Open WebUI Demo](./demo.gif)

## Hauptmerkmale von Open WebUI ‚≠ê

- üöÄ **M√ºhelose Einrichtung**: Installiere nahtlos √ºber Docker oder Kubernetes (kubectl, kustomize oder helm) f√ºr eine problemlose Erfahrung mit Unterst√ºtzung f√ºr `:ollama`- und `:cuda`-getaggte Images.

- ü§ù **Ollama/OpenAI API-Integration**: Integriere m√ºhelos OpenAI-kompatible APIs f√ºr vielseitige Konversationen neben Ollama-Modellen. Passe die OpenAI API-URL an, um sie mit **LMStudio, GroqCloud, Mistral, OpenRouter und mehr** zu verkn√ºpfen.

- üß© **Pipelines, Open WebUI Plugin-Support**: Integriere benutzerdefinierte Logik und Python-Bibliotheken nahtlos in Open WebUI mit dem [Pipelines Plugin Framework](https://github.com/open-webui/pipelines). Starte deine Pipelines-Instanz, setze die OpenAI-URL auf die Pipelines-URL und erkunde endlose M√∂glichkeiten. [Beispiele](https://github.com/open-webui/pipelines/tree/main/examples) umfassen **Function Calling**, Benutzer-**Rate-Limiting** zur Zugriffssteuerung, **Nutzungs√ºberwachung** mit Tools wie Langfuse, **Live-√úbersetzung mit LibreTranslate** f√ºr mehrsprachige Unterst√ºtzung, **Filterung toxischer Nachrichten** und vieles mehr.

- üì± **Responsive Design**: Genie√üe ein nahtloses Erlebnis auf Desktop-PCs, Laptops und Mobilger√§ten.

- üì± **Progressive Web App (PWA) f√ºr Mobilger√§te**: Genie√üe ein naturnahes App-Erlebnis auf deinem Mobilger√§t mit unserer PWA, die Offline-Zugriff auf localhost und eine nahtlose Benutzeroberfl√§che bietet.

- ‚úíÔ∏èüî¢ **Vollst√§ndige Markdown- und LaTeX-Unterst√ºtzung**: Hebe dein LLM-Erlebnis mit umfassender Markdown- und LaTeX-Unterst√ºtzung f√ºr eine bereicherte Interaktion auf eine neue Stufe.

- üé§üìπ **Freih√§ndige Sprach-/Videoanrufe**: Erlebe nahtlose Kommunikation mit integrierten freih√§ndigen Sprach- und Videoanruffunktionen, die eine dynamischere und interaktivere Chat-Umgebung erm√∂glichen.

- üõ†Ô∏è **Modell-Builder**: Erstelle Ollama-Modelle m√ºhelos √ºber die Web-Oberfl√§che. Erstelle und f√ºge benutzerdefinierte Charaktere/Agenten hinzu, passe Chatelemente an und importiere Modelle problemlos √ºber die Integration der [Open WebUI Community](https://openwebui.com/).

- üêç **Native Python-Funktionsaufrufe**: Erweitere deine LLMs mit eingebauter Unterst√ºtzung f√ºr Code-Editoren im Tool-Workspace. Bring Your Own Function (BYOF) einfach durch Hinzuf√ºgen deiner reinen Python-Funktionen und erm√∂gliche nahtlose Integration mit LLMs.

- üìö **Lokale RAG-Integration**: Tauche ein in die Zukunft der Chat-Interaktionen mit bahnbrechender Retrieval Augmented Generation (RAG)-Unterst√ºtzung. Diese Funktion integriert Dokumenteninteraktionen nahtlos in dein Chaterlebnis. Du kannst Dokumente direkt in den Chat laden oder Dateien zu deiner Dokumentenbibliothek hinzuf√ºgen und sie m√ºhelos mit dem `#`-Befehl vor einer Abfrage abrufen.

- üîç **Websuche f√ºr RAG**: F√ºhre Websuchen mit Anbietern wie `SearXNG`, `Google PSE`, `Brave Search`, `serpstack`, `serper`, `Serply`, `DuckDuckGo` und `TavilySearch` durch und f√ºge die Ergebnisse direkt in dein Chaterlebnis ein.

- üåê **Web-Browsing-Funktion**: Integriere nahtlos Websites in dein Chaterlebnis mit dem `#`-Befehl gefolgt von einer URL. Diese Funktion erm√∂glicht es dir, Webinhalte direkt in deine Gespr√§che einzubinden, was die Reichhaltigkeit und Tiefe deiner Interaktionen erh√∂ht.

- üé® **Bildgenerierungs-Integration**: Integriere nahtlos Bildgenerierungsfunktionen mit Optionen wie AUTOMATIC1111 API oder ComfyUI (lokal) und OpenAI's DALL-E (extern) und bereichere dein Chaterlebnis mit dynamischen visuellen Inhalten.

- ‚öôÔ∏è **Vielzahl von Modell-Konversationen**: F√ºhre m√ºhelos Gespr√§che mit verschiedenen Modellen gleichzeitig, nutze deren einzigartige St√§rken f√ºr optimale Antworten. Verbessere dein Erlebnis, indem du eine vielf√§ltige Auswahl an Modellen parallel einsetzt.

- üîê **Rollenbasierte Zugriffssteuerung (RBAC)**: Stelle sicheren Zugriff mit eingeschr√§nkten Berechtigungen sicher; nur autorisierte Personen k√∂nnen auf dein Ollama zugreifen, und exklusive Rechte zur Modellerstellung/zum Modellzugriff sind Administratoren vorbehalten.

- üåêüåç **Mehrsprachige Unterst√ºtzung**: Erlebe Open WebUI in deiner bevorzugten Sprache mit unserer Internationalisierungsunterst√ºtzung (i18n). Schlie√üe dich uns an, um unsere unterst√ºtzten Sprachen zu erweitern! Wir suchen aktiv nach Mitwirkenden!

- üåü **Kontinuierliche Updates**: Wir sind bestrebt, Open WebUI mit regelm√§√üigen Updates, Fehlerbehebungen und neuen Funktionen zu verbessern.

M√∂chtest du mehr √ºber die Funktionen von Open WebUI erfahren? Schaue dir unsere [Open WebUI-Dokumentation](https://docs.openwebui.com/features) f√ºr einen umfassenden √úberblick an!

## üîó Sieh dir auch die Open WebUI Community an!

Vergiss nicht, unser Schwesterprojekt, die [Open WebUI Community](https://openwebui.com/), zu erkunden, wo du benutzerdefinierte Modelfiles entdecken, herunterladen und erkunden kannst. Die Open WebUI Community bietet eine breite Palette aufregender M√∂glichkeiten, um deine Chat-Interaktionen mit Open WebUI zu verbessern! üöÄ

## So installierst du üöÄ

> [!HINWEIS]  
> Bitte beachte, dass f√ºr bestimmte Docker-Umgebungen zus√§tzliche Konfigurationen erforderlich sein k√∂nnen. Wenn du auf Verbindungsprobleme st√∂√üt, steht dir unser detaillierter Leitfaden in der [Open WebUI-Dokumentation](https://docs.openwebui.com/) zur Verf√ºgung.

### Schneller Start mit Docker üê≥

> [!WARNUNG]
> Wenn du Docker verwendest, um Open WebUI zu installieren, achte darauf, das `-v open-webui:/app/backend/data` in deinem Docker-Befehl einzuschlie√üen. Dieser Schritt ist entscheidend, da er sicherstellt, dass deine Datenbank ordnungsgem√§√ü eingebunden ist und Datenverluste verhindert werden.

> [!TIPP]  
> Wenn du Open WebUI mit eingeschlossenem Ollama oder CUDA-Beschleunigung nutzen m√∂chtest, empfehlen wir die Verwendung unserer offiziellen Images, die entweder mit `:cuda` oder `:ollama` getaggt sind. Um CUDA zu aktivieren, musst du das [Nvidia CUDA-Container-Toolkit](https://docs.nvidia.com/dgx/nvidia-container-runtime-upgrade/) auf deinem Linux-/WSL-System installieren.

### Installation mit Standardkonfiguration

- **Wenn Ollama auf deinem Computer installiert ist**, verwende diesen Befehl:

  ```bash
  docker run

 -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
  ```

- **Wenn Ollama auf einem anderen Server installiert ist**, verwende diesen Befehl:

  Um eine Verbindung zu Ollama auf einem anderen Server herzustellen, √§ndere die `OLLAMA_BASE_URL` auf die URL des Servers:

  ```bash
  docker run -d -p 3000:8080 -e OLLAMA_BASE_URL=https://example.com -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
  ```

  - **Um Open WebUI mit Nvidia GPU-Unterst√ºtzung auszuf√ºhren**, verwende diesen Befehl:

  ```bash
  docker run -d -p 3000:8080 --gpus all --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:cuda
  ```

### Installation f√ºr die ausschlie√üliche Nutzung der OpenAI API

- **Wenn du nur die OpenAI API verwendest**, verwende diesen Befehl:

  ```bash
  docker run -d -p 3000:8080 -e OPENAI_API_KEY=dein_geheimer_schl√ºssel -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
  ```

### Installation von Open WebUI mit geb√ºndelter Ollama-Unterst√ºtzung

Diese Installationsmethode verwendet ein einzelnes Container-Image, das Open WebUI mit Ollama b√ºndelt, um eine vereinfachte Einrichtung √ºber einen einzigen Befehl zu erm√∂glichen. W√§hle den passenden Befehl basierend auf deiner Hardwarekonfiguration:

- **Mit GPU-Unterst√ºtzung**:
  Nutze GPU-Ressourcen, indem du den folgenden Befehl ausf√ºhrst:

  ```bash
  docker run -d -p 3000:8080 --gpus=all -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama
  ```

- **Nur f√ºr CPU**:
  Wenn du keine GPU verwendest, verwende stattdessen diesen Befehl:

  ```bash
  docker run -d -p 3000:8080 -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama
  ```

Beide Befehle erm√∂glichen eine integrierte, m√ºhelose Installation sowohl von Open WebUI als auch Ollama und stellen sicher, dass du alles schnell zum Laufen bringen kannst.

Nach der Installation kannst du Open WebUI unter [http://localhost:3000](http://localhost:3000) aufrufen. Viel Spa√ü! üòÑ

### Andere Installationsmethoden

Wir bieten verschiedene Installationsalternativen, einschlie√ülich nicht-Docker-native Installationsmethoden, Docker Compose, Kustomize und Helm. Besuche unsere [Open WebUI-Dokumentation](https://docs.openwebui.com/getting-started/) oder trete unserer [Discord-Community](https://discord.gg/5rJgQTnV4s) bei, um umfassende Anleitungen zu erhalten.

### Fehlerbehebung

Treten Verbindungsprobleme auf? Unsere [Open WebUI-Dokumentation](https://docs.openwebui.com/troubleshooting/) hilft dir weiter. F√ºr zus√§tzliche Unterst√ºtzung und um unserer lebendigen Community beizutreten, besuche den [Open WebUI Discord](https://discord.gg/5rJgQTnV4s).

#### Open WebUI: Serververbindungsfehler

Wenn du auf Verbindungsprobleme st√∂√üt, liegt dies h√§ufig daran, dass der WebUI-Docker-Container den Ollama-Server unter 127.0.0.1:11434 (host.docker.internal:11434) im Container nicht erreichen kann. Verwende das `--network=host`-Flag in deinem Docker-Befehl, um dies zu beheben. Beachte, dass sich der Port von 3000 auf 8080 √§ndert, was zu folgendem Link f√ºhrt: `http://localhost:8080`.

**Beispiel Docker-Befehl**:

```bash
docker run -d --network=host -v open-webui:/app/backend/data -e OLLAMA_BASE_URL=http://127.0.0.1:11434 --name open-webui --restart always ghcr.io/open-webui/open-webui:main
```

### Halte deine Docker-Installation auf dem neuesten Stand

Falls du deine lokale Docker-Installation auf die neueste Version aktualisieren m√∂chtest, kannst du dies mit [Watchtower](https://containrrr.dev/watchtower/) tun:

```bash
docker run --rm --volume /var/run/docker.sock:/var/run/docker.sock containrrr/watchtower --run-once open-webui
```

Ersetze im letzten Teil des Befehls `open-webui` durch deinen Container-Namen, falls dieser anders ist.

Sieh dir unseren Migrationsleitfaden in unserer [Open WebUI-Dokumentation](https://docs.openwebui.com/migration/) an.

### Nutzung des Dev-Branch üåô

> [!WARNUNG]
> Der `:dev`-Branch enth√§lt die neuesten instabilen Funktionen und √Ñnderungen. Verwende ihn auf eigenes Risiko, da er Fehler oder unvollst√§ndige Funktionen enthalten kann.

Wenn du die neuesten, allerneuesten Funktionen ausprobieren m√∂chtest und mit gelegentlicher Instabilit√§t einverstanden bist, kannst du den `:dev`-Tag wie folgt verwenden:

```bash
docker run -d -p 3000:8080 -v open-webui:/app/backend/data --name open-webui --add-host=host.docker.internal:host-gateway --restart always ghcr.io/open-webui/open-webui:dev
```

## Was kommt als n√§chstes? üåü

Entdecke kommende Funktionen auf unserer Roadmap in der [Open WebUI-Dokumentation](https://docs.openwebui.com/roadmap/).

## Unterst√ºtzer ‚ú®

Ein gro√ües Dankesch√∂n an unsere gro√üartigen Unterst√ºtzer, die dieses Projekt m√∂glich machen! üôè

### Platin-Sponsoren ü§ç

- Wir suchen nach Sponsoren!

### Anerkennungen

Besonderer Dank gilt [Prof. Lawrence Kim](https://www.lhkim.com/) und [Prof. Nick Vincent](https://www.nickmvincent.com/) f√ºr ihre unsch√§tzbare Unterst√ºtzung und Anleitung bei der Gestaltung dieses Projekts zu einem Forschungsprojekt. Dankbar f√ºr eure Mentorschaft w√§hrend des gesamten Weges! üôå

## Lizenz üìú

Dieses Projekt ist unter der [MIT-Lizenz](LICENSE) lizenziert ‚Äì siehe die [LICENSE](LICENSE)-Datei f√ºr Details. üìÑ

## Unterst√ºtzung üí¨

Wenn du Fragen, Vorschl√§ge oder Hilfe ben√∂tigst, √∂ffne bitte ein Issue oder tritt unserer
[Open WebUI Discord-Community](https://discord.gg/5rJgQTnV4s) bei, um dich mit uns zu verbinden! ü§ù

## Stern-Historie

<a href="https://star-history.com/#open-webui/open-webui&Date">
  <picture>
    <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=open-webui/open-webui&type=Date&theme=dark" />
    <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=open-webui/open-webui&type=Date" />
    <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=open-webui/open-webui&type=Date" />
  </picture>
</a>

---

Erstellt von [Timothy J. Baek](https://github.com/tjbck) - Lass uns gemeinsam Open WebUI noch gro√üartiger machen! üí™
