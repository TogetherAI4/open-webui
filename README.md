# Open WebUI (FrÃ¼her Ollama WebUI) ğŸ‘‹

![GitHub Sterne](https://img.shields.io/github/stars/open-webui/open-webui?style=social)
![GitHub Forks](https://img.shields.io/github/forks/open-webui/open-webui?style=social)
![GitHub Beobachter](https://img.shields.io/github/watchers/open-webui/open-webui?style=social)
![GitHub Repository GrÃ¶ÃŸe](https://img.shields.io/github/repo-size/open-webui/open-webui)
![GitHub Sprachenanzahl](https://img.shields.io/github/languages/count/open-webui/open-webui)
![GitHub Top-Sprache](https://img.shields.io/github/languages/top/open-webui/open-webui)
![GitHub letzter Commit](https://img.shields.io/github/last-commit/open-webui/open-webui?color=red)
![Hits](https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2Follama-webui%2Follama-wbui&count_bg=%2379C83D&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=hits&edge_flat=false)
[![Discord](https://img.shields.io/badge/Discord-Open_WebUI-blue?logo=discord&logoColor=white)](https://discord.gg/5rJgQTnV4s)
[![](https://img.shields.io/static/v1?label=Sponsor&message=%E2%9D%A4&logo=GitHub&color=%23fe8e86)](https://github.com/sponsors/tjbck)

Open WebUI ist eine [erweiterbare](https://github.com/open-webui/pipelines), funktionsreiche und benutzerfreundliche selbstgehostete WebUI, die vollstÃ¤ndig offline betrieben werden kann. Sie unterstÃ¼tzt verschiedene LLM-Runner, darunter Ollama- und OpenAI-kompatible APIs. Weitere Informationen findest du in unserer [Open WebUI-Dokumentation](https://docs.openwebui.com/).

![Open WebUI Demo](./demo.gif)

## Hauptmerkmale von Open WebUI â­

- ğŸš€ **MÃ¼helose Einrichtung**: Installiere nahtlos Ã¼ber Docker oder Kubernetes (kubectl, kustomize oder helm) fÃ¼r eine problemlose Erfahrung mit UnterstÃ¼tzung fÃ¼r `:ollama`- und `:cuda`-getaggte Images.

- ğŸ¤ **Ollama/OpenAI API-Integration**: Integriere mÃ¼helos OpenAI-kompatible APIs fÃ¼r vielseitige Konversationen neben Ollama-Modellen. Passe die OpenAI API-URL an, um sie mit **LMStudio, GroqCloud, Mistral, OpenRouter und mehr** zu verknÃ¼pfen.

- ğŸ§© **Pipelines, Open WebUI Plugin-Support**: Integriere benutzerdefinierte Logik und Python-Bibliotheken nahtlos in Open WebUI mit dem [Pipelines Plugin Framework](https://github.com/open-webui/pipelines). Starte deine Pipelines-Instanz, setze die OpenAI-URL auf die Pipelines-URL und erkunde endlose MÃ¶glichkeiten. [Beispiele](https://github.com/open-webui/pipelines/tree/main/examples) umfassen **Function Calling**, Benutzer-**Rate-Limiting** zur Zugriffssteuerung, **NutzungsÃ¼berwachung** mit Tools wie Langfuse, **Live-Ãœbersetzung mit LibreTranslate** fÃ¼r mehrsprachige UnterstÃ¼tzung, **Filterung toxischer Nachrichten** und vieles mehr.

- ğŸ“± **Responsive Design**: GenieÃŸe ein nahtloses Erlebnis auf Desktop-PCs, Laptops und MobilgerÃ¤ten.

- ğŸ“± **Progressive Web App (PWA) fÃ¼r MobilgerÃ¤te**: GenieÃŸe ein naturnahes App-Erlebnis auf deinem MobilgerÃ¤t mit unserer PWA, die Offline-Zugriff auf localhost und eine nahtlose BenutzeroberflÃ¤che bietet.

- âœ’ï¸ğŸ”¢ **VollstÃ¤ndige Markdown- und LaTeX-UnterstÃ¼tzung**: Hebe dein LLM-Erlebnis mit umfassender Markdown- und LaTeX-UnterstÃ¼tzung fÃ¼r eine bereicherte Interaktion auf eine neue Stufe.

- ğŸ¤ğŸ“¹ **FreihÃ¤ndige Sprach-/Videoanrufe**: Erlebe nahtlose Kommunikation mit integrierten freihÃ¤ndigen Sprach- und Videoanruffunktionen, die eine dynamischere und interaktivere Chat-Umgebung ermÃ¶glichen.

- ğŸ› ï¸ **Modell-Builder**: Erstelle Ollama-Modelle mÃ¼helos Ã¼ber die Web-OberflÃ¤che. Erstelle und fÃ¼ge benutzerdefinierte Charaktere/Agenten hinzu, passe Chatelemente an und importiere Modelle problemlos Ã¼ber die Integration der [Open WebUI Community](https://openwebui.com/).

- ğŸ **Native Python-Funktionsaufrufe**: Erweitere deine LLMs mit eingebauter UnterstÃ¼tzung fÃ¼r Code-Editoren im Tool-Workspace. Bring Your Own Function (BYOF) einfach durch HinzufÃ¼gen deiner reinen Python-Funktionen und ermÃ¶gliche nahtlose Integration mit LLMs.

- ğŸ“š **Lokale RAG-Integration**: Tauche ein in die Zukunft der Chat-Interaktionen mit bahnbrechender Retrieval Augmented Generation (RAG)-UnterstÃ¼tzung. Diese Funktion integriert Dokumenteninteraktionen nahtlos in dein Chaterlebnis. Du kannst Dokumente direkt in den Chat laden oder Dateien zu deiner Dokumentenbibliothek hinzufÃ¼gen und sie mÃ¼helos mit dem `#`-Befehl vor einer Abfrage abrufen.

- ğŸ” **Websuche fÃ¼r RAG**: FÃ¼hre Websuchen mit Anbietern wie `SearXNG`, `Google PSE`, `Brave Search`, `serpstack`, `serper`, `Serply`, `DuckDuckGo` und `TavilySearch` durch und fÃ¼ge die Ergebnisse direkt in dein Chaterlebnis ein.

- ğŸŒ **Web-Browsing-Funktion**: Integriere nahtlos Websites in dein Chaterlebnis mit dem `#`-Befehl gefolgt von einer URL. Diese Funktion ermÃ¶glicht es dir, Webinhalte direkt in deine GesprÃ¤che einzubinden, was die Reichhaltigkeit und Tiefe deiner Interaktionen erhÃ¶ht.

- ğŸ¨ **Bildgenerierungs-Integration**: Integriere nahtlos Bildgenerierungsfunktionen mit Optionen wie AUTOMATIC1111 API oder ComfyUI (lokal) und OpenAI's DALL-E (extern) und bereichere dein Chaterlebnis mit dynamischen visuellen Inhalten.

- âš™ï¸ **Vielzahl von Modell-Konversationen**: FÃ¼hre mÃ¼helos GesprÃ¤che mit verschiedenen Modellen gleichzeitig, nutze deren einzigartige StÃ¤rken fÃ¼r optimale Antworten. Verbessere dein Erlebnis, indem du eine vielfÃ¤ltige Auswahl an Modellen parallel einsetzt.

- ğŸ” **Rollenbasierte Zugriffssteuerung (RBAC)**: Stelle sicheren Zugriff mit eingeschrÃ¤nkten Berechtigungen sicher; nur autorisierte Personen kÃ¶nnen auf dein Ollama zugreifen, und exklusive Rechte zur Modellerstellung/zum Modellzugriff sind Administratoren vorbehalten.

- ğŸŒğŸŒ **Mehrsprachige UnterstÃ¼tzung**: Erlebe Open WebUI in deiner bevorzugten Sprache mit unserer InternationalisierungsunterstÃ¼tzung (i18n). SchlieÃŸe dich uns an, um unsere unterstÃ¼tzten Sprachen zu erweitern! Wir suchen aktiv nach Mitwirkenden!

- ğŸŒŸ **Kontinuierliche Updates**: Wir sind bestrebt, Open WebUI mit regelmÃ¤ÃŸigen Updates, Fehlerbehebungen und neuen Funktionen zu verbessern.

MÃ¶chtest du mehr Ã¼ber die Funktionen von Open WebUI erfahren? Schaue dir unsere [Open WebUI-Dokumentation](https://docs.openwebui.com/features) fÃ¼r einen umfassenden Ãœberblick an!

## ğŸ”— Sieh dir auch die Open WebUI Community an!

Vergiss nicht, unser Schwesterprojekt, die [Open WebUI Community](https://openwebui.com/), zu erkunden, wo du benutzerdefinierte Modelfiles entdecken, herunterladen und erkunden kannst. Die Open WebUI Community bietet eine breite Palette aufregender MÃ¶glichkeiten, um deine Chat-Interaktionen mit Open WebUI zu verbessern! ğŸš€

## So installierst du ğŸš€

> [!HINWEIS]  
> Bitte beachte, dass fÃ¼r bestimmte Docker-Umgebungen zusÃ¤tzliche Konfigurationen erforderlich sein kÃ¶nnen. Wenn du auf Verbindungsprobleme stÃ¶ÃŸt, steht dir unser detaillierter Leitfaden in der [Open WebUI-Dokumentation](https://docs.openwebui.com/) zur VerfÃ¼gung.

### Schneller Start mit Docker ğŸ³

> [!WARNUNG]
> Wenn du Docker verwendest, um Open WebUI zu installieren, achte darauf, das `-v open-webui:/app/backend/data` in deinem Docker-Befehl einzuschlieÃŸen. Dieser Schritt ist entscheidend, da er sicherstellt, dass deine Datenbank ordnungsgemÃ¤ÃŸ eingebunden ist und Datenverluste verhindert werden.

> [!TIPP]  
> Wenn du Open WebUI mit eingeschlossenem Ollama oder CUDA-Beschleunigung nutzen mÃ¶chtest, empfehlen wir die Verwendung unserer offiziellen Images, die entweder mit `:cuda` oder `:ollama` getaggt sind. Um CUDA zu aktivieren, musst du das [Nvidia CUDA-Container-Toolkit](https://docs.nvidia.com/dgx/nvidia-container-runtime-upgrade/) auf deinem Linux-/WSL-System installieren.

### Installation mit Standardkonfiguration

- **Wenn Ollama auf deinem Computer installiert ist**, verwende diesen Befehl:

  ```bash
  docker run

 -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
  ```

- **Wenn Ollama auf einem anderen Server installiert ist**, verwende diesen Befehl:

  Um eine Verbindung zu Ollama auf einem anderen Server herzustellen, Ã¤ndere die `OLLAMA_BASE_URL` auf die URL des Servers:

  ```bash
  docker run -d -p 3000:8080 -e OLLAMA_BASE_URL=https://example.com -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
  ```

  - **Um Open WebUI mit Nvidia GPU-UnterstÃ¼tzung auszufÃ¼hren**, verwende diesen Befehl:

  ```bash
  docker run -d -p 3000:8080 --gpus all --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:cuda
  ```

### Installation fÃ¼r die ausschlieÃŸliche Nutzung der OpenAI API

- **Wenn du nur die OpenAI API verwendest**, verwende diesen Befehl:

  ```bash
  docker run -d -p 3000:8080 -e OPENAI_API_KEY=dein_geheimer_schlÃ¼ssel -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
  ```

### Installation von Open WebUI mit gebÃ¼ndelter Ollama-UnterstÃ¼tzung

Diese Installationsmethode verwendet ein einzelnes Container-Image, das Open WebUI mit Ollama bÃ¼ndelt, um eine vereinfachte Einrichtung Ã¼ber einen einzigen Befehl zu ermÃ¶glichen. WÃ¤hle den passenden Befehl basierend auf deiner Hardwarekonfiguration:

- **Mit GPU-UnterstÃ¼tzung**:
  Nutze GPU-Ressourcen, indem du den folgenden Befehl ausfÃ¼hrst:

  ```bash
  docker run -d -p 3000:8080 --gpus=all -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama
  ```

- **Nur fÃ¼r CPU**:
  Wenn du keine GPU verwendest, verwende stattdessen diesen Befehl:

  ```bash
  docker run -d -p 3000:8080 -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama
  ```

Beide Befehle ermÃ¶glichen eine integrierte, mÃ¼helose Installation sowohl von Open WebUI als auch Ollama und stellen sicher, dass du alles schnell zum Laufen bringen kannst.

Nach der Installation kannst du Open WebUI unter [http://localhost:3000](http://localhost:3000) aufrufen. Viel SpaÃŸ! ğŸ˜„

### Andere Installationsmethoden

Wir bieten verschiedene Installationsalternativen, einschlieÃŸlich nicht-Docker-native Installationsmethoden, Docker Compose, Kustomize und Helm. Besuche unsere [Open WebUI-Dokumentation](https://docs.openwebui.com/getting-started/) oder trete unserer [Discord-Community](https://discord.gg/5rJgQTnV4s) bei, um umfassende Anleitungen zu erhalten.

### Fehlerbehebung

Treten Verbindungsprobleme auf? Unsere [Open WebUI-Dokumentation](https://docs.openwebui.com/troubleshooting/) hilft dir weiter. FÃ¼r zusÃ¤tzliche UnterstÃ¼tzung und um unserer lebendigen Community beizutreten, besuche den [Open WebUI Discord](https://discord.gg/5rJgQTnV4s).

#### Open WebUI: Serververbindungsfehler

Wenn du auf Verbindungsprobleme stÃ¶ÃŸt, liegt dies hÃ¤ufig daran, dass der WebUI-Docker-Container den Ollama-Server unter 127.0.0.1:11434 (host.docker.internal:11434) im Container nicht erreichen kann. Verwende das `--network=host`-Flag in deinem Docker-Befehl, um dies zu beheben. Beachte, dass sich der Port von 3000 auf 8080 Ã¤ndert, was zu folgendem Link fÃ¼hrt: `http://localhost:8080`.

**Beispiel Docker-Befehl**:

```bash
docker run -d --network=host -v open-webui:/app/backend/data -e OLLAMA_BASE_URL=http://127.0.0.1:11434 --name open-webui --restart always ghcr.io/open-webui/open-webui:main
```

### Halte deine Docker-Installation auf dem neuesten Stand

Falls du deine lokale Docker-Installation auf die neueste Version aktualisieren mÃ¶chtest, kannst du dies mit [Watchtower](https://containrrr.dev/watchtower/) tun:

```bash
docker run --rm --volume /var/run/docker.sock:/var/run/docker.sock containrrr/watchtower --run-once open-webui
```

Ersetze im letzten Teil des Befehls `open-webui` durch deinen Container-Namen, falls dieser anders ist.

Sieh dir unseren Migrationsleitfaden in unserer [Open WebUI-Dokumentation](https://docs.openwebui.com/migration/) an.

### Nutzung des Dev-Branch ğŸŒ™

> [!WARNUNG]
> Der `:dev`-Branch enthÃ¤lt die neuesten instabilen Funktionen und Ã„nderungen. Verwende ihn auf eigenes Risiko, da er Fehler oder unvollstÃ¤ndige Funktionen enthalten kann.

Wenn du die neuesten, allerneuesten Funktionen ausprobieren mÃ¶chtest und mit gelegentlicher InstabilitÃ¤t einverstanden bist, kannst du den `:dev`-Tag wie folgt verwenden:

```bash
docker run -d -p 3000:8080 -v open-webui:/app/backend/data --name open-webui --add-host=host.docker.internal:host-gateway --restart always ghcr.io/open-webui/open-webui:dev
```

## Was kommt als nÃ¤chstes? ğŸŒŸ

Entdecke kommende Funktionen auf unserer Roadmap in der [Open WebUI-Dokumentation](https://docs.openwebui.com/roadmap/).

## UnterstÃ¼tzer âœ¨

Ein groÃŸes DankeschÃ¶n an unsere groÃŸartigen UnterstÃ¼tzer, die dieses Projekt mÃ¶glich machen! ğŸ™

### Platin-Sponsoren ğŸ¤

- Wir suchen nach Sponsoren!

### Anerkennungen

Besonderer Dank gilt [Prof. Lawrence Kim](https://www.lhkim.com/) und [Prof. Nick Vincent](https://www.nickmvincent.com/) fÃ¼r ihre unschÃ¤tzbare UnterstÃ¼tzung und Anleitung bei der Gestaltung dieses Projekts zu einem Forschungsprojekt. Dankbar fÃ¼r eure Mentorschaft wÃ¤hrend des gesamten Weges! ğŸ™Œ

## Lizenz ğŸ“œ

Dieses Projekt ist unter der [MIT-Lizenz](LICENSE) lizenziert â€“ siehe die [LICENSE](LICENSE)-Datei fÃ¼r Details. ğŸ“„

## UnterstÃ¼tzung ğŸ’¬

Wenn du Fragen, VorschlÃ¤ge oder Hilfe benÃ¶tigst, Ã¶ffne bitte ein Issue oder tritt unserer
[Open WebUI Discord-Community](https://discord.gg/5rJgQTnV4s) bei, um dich mit uns zu verbinden! ğŸ¤

## Stern-Historie

<a href="https://star-history.com/#open-webui/open-webui&Date">
  <picture>
    <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=open-webui/open-webui&type=Date&theme=dark" />
    <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=open-webui/open-webui&type=Date" />
    <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=open-webui/open-webui&type=Date" />
  </picture>
</a>

---

Erstellt von [Timothy J. Baek](https://github.com/tjbck) - Lass uns gemeinsam Open WebUI noch groÃŸartiger machen! ğŸ’ª
